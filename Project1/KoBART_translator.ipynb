{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "from transformers import BartTokenizer\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "  def __init__(self, df, tokenizer, max_len, ignore_index=-100, verbose=True):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    self.df = df\n",
    "    self.len = len(self.df)\n",
    "    self.pad_index = self.tokenizer.pad_token_id\n",
    "    self.ignore_index = ignore_index\n",
    "\n",
    "  def add_padding_data(self, inputs):\n",
    "    if len(inputs) < self.max_len:\n",
    "      pad = np.array([self.pad_index] * (self.max_len - len(inputs)))\n",
    "      inputs = np.concatenate([inputs, pad])\n",
    "    else:\n",
    "      inputs = inputs[:self.max_len]\n",
    "    return inputs\n",
    "\n",
    "  def add_ignored_data(self, inputs):\n",
    "    if len(inputs) < self.max_len:\n",
    "      pad = np.array([self.ignore_index] * (self.max_len - len(inputs)))\n",
    "      inputs = np.concatenate([inputs, pad])\n",
    "    else:\n",
    "      inputs = inputs[:self.max_len]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "  def __getitem__(self, idx, verbose=True):\n",
    "    instance = self.df.iloc[idx]\n",
    "    input_ids = self.tokenizer.encode(instance['원문'])\n",
    "    input_ids = np.append(input_ids, self.tokenizer.eos_token_id)\n",
    "    input_ids = self.add_padding_data(input_ids)\n",
    "    input_ids = np.insert(input_ids, 0, self.tokenizer.bos_token_id)\n",
    "\n",
    "    label_ids = self.tokenizer.encode(instance['번역문'])\n",
    "    label_ids.append(self.tokenizer.eos_token_id)\n",
    "    label_ids.insert(0, self.tokenizer.bos_token_id)\n",
    "\n",
    "    dec_input_ids = [self.tokenizer.eos_token_id]\n",
    "    dec_input_ids += label_ids[:-1]\n",
    "    dec_input_ids = self.add_padding_data(dec_input_ids)\n",
    "    label_ids = self.add_ignored_data(label_ids)\n",
    "\n",
    "    input_ids = torch.tensor(np.array(input_ids)).long()\n",
    "    decoder_input_ids = torch.tensor(np.array(dec_input_ids)).long()\n",
    "\n",
    "    attention_mask = input_ids.ne(self.tokenizer.pad_token_id).float()\n",
    "\n",
    "    return {'input_ids': input_ids,\n",
    "            #'attention_mask': input_ids.ne(self.tokenizer.pad_token_id).float(),\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            # 'decoder_attention_mask': decoder_input_ids.ne(self.tokenizer.pad_token_id).float(),\n",
    "            'labels': np.array(label_ids, dtype = np.int_)}\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  preds, labels = pred\n",
    "\n",
    "  preds = tokenizer.batch_decode(preds, skip_special_tokens = True)\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  labels = tokenizer.batch_decode(labels, skip_special_tokens = True)\n",
    "\n",
    "  print(\"원문: \", val['원문'][0])\n",
    "  print(\"번역 정답\", labels[0])\n",
    "  print(\"번역 결과: \", preds[0])\n",
    "\n",
    "  reference = preds[0].split()\n",
    "  candidate = []\n",
    "  candidate.append(labels[0].split())\n",
    "  bleu = sentence_bleu(references = candidate, hypothesis=reference, weights=(1, 0, 0, 0))\n",
    "  return {\"BLEU score\": bleu }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "stop = 3\n",
    "epoch = 10\n",
    "batch = 4\n",
    "seed = 42\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"english_korean_data/train_small.csv\", encoding=\"cp949\")\n",
    "val = pd.read_csv(\"english_korean_data/test_open.csv\", encoding=\"cp949\")\n",
    "train_dataset = TranslationDataset(train, PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1'), 256)\n",
    "val_dataset = TranslationDataset(val, PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1'), 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id = tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "\n",
    "\n",
    "config = BartConfig.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "\n",
    "\n",
    "config.encoder_embed_dim = 768  \n",
    "config.encoder_embed_path = None\n",
    "\n",
    "\n",
    "encoder_embedding = torch.nn.Embedding(config.vocab_size, config.encoder_embed_dim)\n",
    "\n",
    "\n",
    "original_model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v1\")\n",
    "\n",
    "\n",
    "original_model.model.encoder.embed_tokens = encoder_embedding\n",
    "\n",
    "\n",
    "model = original_model\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n",
    "                                                        num_warmup_steps = 100,\n",
    "                                                        num_training_steps = epoch * len(train_dataset) * batch,\n",
    "                                                        last_epoch = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(run_name = \"KoBART_translator\",\n",
    "                                output_dir = \"./BART_translator_2\",\n",
    "                                evaluation_strategy=\"steps\",\n",
    "                                eval_steps = 100,\n",
    "                                save_steps = 100,\n",
    "                                save_total_limit=2,\n",
    "\n",
    "                                per_device_train_batch_size= batch,\n",
    "                                per_device_eval_batch_size = batch,\n",
    "                                gradient_accumulation_steps = 16,\n",
    "                                num_train_epochs = epoch,\n",
    "\n",
    "                                load_best_model_at_end = True,\n",
    "                                #fp16=True,\n",
    "                                do_train=True,\n",
    "                                do_eval=True,\n",
    "                                predict_with_generate=True,)\n",
    "\n",
    "trainer = Seq2SeqTrainer(model = model,\n",
    "                        tokenizer = tokenizer,\n",
    "                        args = args,\n",
    "                        train_dataset = train_dataset,\n",
    "                        eval_dataset = val_dataset,\n",
    "                        compute_metrics = compute_metrics,\n",
    "                        optimizers = (optimizer, lr_scheduler),\n",
    "                        data_collator = collator,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,  # 현재 학습 epoch\n",
    "    'model_state_dict': model.state_dict(),  # 모델 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n",
    "}, 'translator3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text, label):\n",
    "  tmp = [tokenizer.bos_token_id] + tokenizer.encode(text) + [tokenizer.eos_token_id]\n",
    "  out = model.generate(input_ids = torch.tensor(tmp)[None, :].to(device))\n",
    "  result = tokenizer.decode(out[0])\n",
    "\n",
    "  print(\"번역 결과: \", result)\n",
    "\n",
    "  reference = result.split()\n",
    "  candidate = []\n",
    "  candidate.append(label.split())\n",
    "  bleu = sentence_bleu(references=candidate, hypothesis=reference, weights=(1, 0, 0, 0))\n",
    "\n",
    "  print(\"BLEU score\", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(\"아쉽지만 그러면 한 명은 기다려야 할 것 같네요.\", \"This room stinks of cigarette smells. I want to change rooms.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
