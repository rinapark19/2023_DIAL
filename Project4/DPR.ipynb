{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRDataset(Dataset):\n",
    "    def __init__(self, passages, questions, p_tokenizer, q_tokenizer):\n",
    "        self.passages = passages\n",
    "        self.questions = questions\n",
    "        self.p_tokenizer = p_tokenizer\n",
    "        self.q_tokenizer = q_tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.passages)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        passage = self.passages[index]\n",
    "        question = self.questions[index]\n",
    "        return passage, question\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        passages, questions = zip(*batch)\n",
    "        passage_inputs = self.p_tokenizer.batch_encode_plus(passages, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        question_inputs = self.q_tokenizer.batch_encode_plus(questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return passage_inputs, question_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model & tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "q_encoder = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "p_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "q_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, optimizer, scheduler, device, batch):\n",
    "    p_encoder.train()\n",
    "    q_encoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for passage_inputs, question_inputs in pbar:\n",
    "        passage_inputs = {k: v.to(device) for k, v in passage_inputs.items()}\n",
    "        question_inputs = {k: v.to(device) for k, v in question_inputs.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        passage_embeddings = p_encoder(**passage_inputs).pooler_output\n",
    "        question_embeddings = q_encoder(**question_inputs).pooler_output\n",
    "\n",
    "        sim_scores = torch.matmul(question_embeddings, torch.transpose(passage_embeddings, 0, 1))\n",
    "\n",
    "        targets = torch.arange(0, batch).long().to(device)\n",
    "\n",
    "        sim_scores = torch.nn.functional.log_softmax(sim_scores, dim=1)\n",
    "        loss = torch.nn.functional.nll_loss(sim_scores, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix({\"Loss\" : loss.item()})\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-5\n",
    "warmup_steps = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "traindata = pd.read_csv(\"train.csv\")\n",
    "question = pd.read_csv(\"question.csv\")\n",
    "collection = pd.read_csv(\"collection.csv\")\n",
    "evaldata = pd.read_csv(\"test.csv\")\n",
    "\n",
    "question.columns = ['0', 'x_id', 'content']\n",
    "collection.columns = ['0', 'y_id', 'document']\n",
    "\n",
    "traindata = traindata.merge(question, on=\"x_id\", how=\"left\")\n",
    "\n",
    "traindata.dropna(inplace = True)\n",
    "traindata.reset_index(inplace = True, drop=True)\n",
    "\n",
    "traindata = traindata.merge(collection, on=\"y_id\", how=\"left\")\n",
    "traindata.dropna(inplace=True)\n",
    "traindata.reset_index(inplace=True, drop=True)\n",
    "\n",
    "traindata = traindata[[\"content\", \"document\"]]\n",
    "traindata.columns = [\"question\", \"context\"]\n",
    "\n",
    "evaldata = evaldata.merge(question, on=\"x_id\", how=\"left\")\n",
    "\n",
    "evaldata.dropna(inplace = True)\n",
    "evaldata.reset_index(inplace = True, drop=True)\n",
    "\n",
    "evaldata = evaldata.merge(collection, on=\"y_id\", how=\"left\")\n",
    "evaldata.dropna(inplace=True)\n",
    "evaldata.reset_index(inplace=True, drop=True)\n",
    "\n",
    "evaldata = evaldata[[\"content\", \"document\"]]\n",
    "evaldata.columns = [\"question\", \"context\"]\n",
    "\n",
    "train_data = traindata[:50000]\n",
    "eval_data = evaldata\n",
    "\n",
    "train_passages = list(train_data[\"context\"])\n",
    "train_questions = list(train_data[\"question\"])\n",
    "train_dataset = DPRDataset(train_passages, train_questions, p_tokenizer, q_tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "eval_passages = list(eval_data[\"context\"])\n",
    "eval_questions = list(eval_data[\"question\"])\n",
    "eval_dataset = DPRDataset(eval_passages, eval_questions, p_tokenizer, q_tokenizer)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=eval_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters =[\n",
    "    {'params': [p for n, p in p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in p_encoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in q_encoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps= total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder.to(device)\n",
    "q_encoder.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(train_dataloader, optimizer, scheduler, device, batch_size)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = pd.read_csv(\"collection.csv\")\n",
    "documents = list(passages['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p_encoder.eval()\n",
    "\n",
    "    # passage embeddings\n",
    "    p_embs = []\n",
    "    num_documents = len(documents)\n",
    "\n",
    "    for p in tqdm(documents, desc=\"Computing Passage Embeddings\", total=num_documents):\n",
    "        p = p_tokenizer(p, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        p_emb = p_encoder(**p).pooler_output.to(\"cpu\").numpy()\n",
    "        p_embs.append(p_emb)\n",
    "\n",
    "    p_embs = torch.Tensor(p_embs).squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p_encoder.eval()\n",
    "\n",
    "    # passage embeddings\n",
    "    p_embs = []\n",
    "    num_documents = len(documents)\n",
    "\n",
    "    for p in tqdm(documents, desc=\"Computing Passage Embeddings\", total=num_documents):\n",
    "        p = p_tokenizer(p, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        p_emb = p_encoder(**p).pooler_output.to(\"cpu\").numpy()\n",
    "        p_embs.append(p_emb)\n",
    "\n",
    "    p_embs = torch.Tensor(p_embs).squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    recall_1 = 0\n",
    "    recall_10 = 0\n",
    "    recall_20 = 0\n",
    "    recall_100 = 0\n",
    "\n",
    "    total_actual_positives = 0\n",
    "\n",
    "    q_encoder.eval()\n",
    "\n",
    "    for sample_idx in tqdm(range(len(eval_questions))):\n",
    "        query = eval_questions[sample_idx]\n",
    "\n",
    "        q_seqs_val = q_tokenizer([query], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        q_emb = q_encoder(**q_seqs_val).pooler_output.to(device)\n",
    "\n",
    "        dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "\n",
    "        rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "\n",
    "        correct_passage = eval_questions[sample_idx]\n",
    "\n",
    "        correct_idx = (documents.index(correct_passage) == rank).nonzero()\n",
    "\n",
    "        if correct_idx.numel() > 0:\n",
    "            correct_idx = correct_idx.item()\n",
    "\n",
    "            if correct_idx < 1:\n",
    "                recall_1 += 1\n",
    "            if correct_idx < 10:\n",
    "                recall_10 += 1\n",
    "            if correct_idx < 20:\n",
    "                recall_20 += 1\n",
    "            if correct_idx < 100:\n",
    "                recall_100 += 1\n",
    "        \n",
    "        total_actual_positives += 1\n",
    "    \n",
    "    recall_1 /= total_actual_positives\n",
    "    recall_10 /= total_actual_positives\n",
    "    recall_20 /= total_actual_positives\n",
    "    recall_100 /= total_actual_positives\n",
    "\n",
    "    print(\"Recall@1: \", recall_1)\n",
    "    print(\"Recall@10: \", recall_10)\n",
    "    print(\"Recall@20: \", recall_20)\n",
    "    print(\"Recall@100\", recall_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model save & load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder.to(\"cpu\")\n",
    "q_encoder.to(\"cpu\")\n",
    "\n",
    "torch.save({\n",
    "    \"p_encoder_state_dict\": p_encoder.state_dict(),\n",
    "    \"q_encoder_state_dict\": q_encoder.state_dict(),\n",
    "}, \"encoder_new.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(\"encoder_new.pt\", map_location=\"cpu\")\n",
    "p_encoder.load_state_dict(model_state_dict[\"p_encoder_state_dict\"])\n",
    "q_encoder.load_state_dict(model_state_dict['q_encoder_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
